{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734ab45a-eac6-46bb-bed7-2eaac6ab701a",
   "metadata": {},
   "source": [
    "# Evaluating Agents' Intermediate Steps\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/testing-examples/agent_steps/evaluating_agents.ipynb)\n",
    "\n",
    "In many scenarios, evaluating an agent isn't merely about the final outcome, but about understanding the steps it took to arrive at that decision. This notebook provides an introductory walkthrough on configuring an evaluator to assess an agent based on its \"decision-making process,\" scoring based on the sequence of selected tools. \n",
    "\n",
    "[![Example Agent Trace](./img/agent_trace.png)](https://smith.langchain.com/public/6d50f517-115f-4c14-97b2-2e19b15efca7/r)\n",
    "\n",
    "We'll [create a custom run evaluator](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators#custom-evaluators-outside-langchain) that captures and compares the intermediate steps of the agent against a pre-defined sequence. This ensures that the agent isn't just providing the correct answers but is also being efficient about how it is using external resources.\n",
    "\n",
    "The basic steps are:\n",
    "\n",
    "- Prepare a dataset with input queries and expected agent actions\n",
    "- Define the agent with specific tools and behavior\n",
    "- Construct custom evaluators that check the actions taken\n",
    "- Running the evaluation\n",
    "\n",
    "Once the evaluation is completed, you can review the results in LangSmith. By the end of this guide, you'll have a better sense of how to apply an evaluator to more complex inputs like an agent's trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fdc4da-d656-47eb-9298-198a5a7a3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain openai --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b84de2b-741e-4236-8b15-6e30edbd295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "INFERENCE_SERVER_URL_Deep = \"http://localhost:8989\"\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MODEL_NAME_Deep = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8000\"\n",
    "MODEL_NAME = \"ibm-granite/granite-3.1-8b-base\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "\n",
    "llm_deepseek = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL_Deep}/v1\",\n",
    "    model_name=MODEL_NAME_Deep,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd1303f-a7bb-4098-bf13-598d9893d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "import os\n",
    "from uuid import uuid4\n",
    "from langsmith import Client\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Evaluating Agents' Intermediate Steps\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526e117",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset\n",
    "\n",
    "Define a new dataset. At minimum, the dataset should have input queries the agent is tasked to solve.\n",
    "We will also store expected steps in our dataset to demonstrate the sequence of actions the agent is expected to\n",
    "take in order to resolve the query.\n",
    "\n",
    "Optionally, you can store reference labels to evaluate the agent's \"correctness\" in an end-to-end fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129154f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['ac6a347b-7689-444f-a10e-495b10111a85',\n",
       "  '8bd29f87-3a71-4695-a850-5c098953729a',\n",
       "  'f5b89bc6-487f-445e-a42d-2f9284792c34',\n",
       "  '1bbde0af-b8f7-411a-8093-bcf1425b430c',\n",
       "  'b2d38b4c-b05e-49a0-9efd-084f7556d726'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "questions = [\n",
    "    (\n",
    "        \"Why was was a $10 calculator app one of the best-rated Nintendo Switch games?\",\n",
    "        {\n",
    "            \"reference\": \"It became an internet meme due to its high price point.\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"hi\",\n",
    "        {\n",
    "            \"reference\": \"Hello, how can I assist you?\",\n",
    "            \"expected_steps\": [],  # Expect a direct response\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who is Dejan Trajkov?\",\n",
    "        {\n",
    "            \"reference\": \"Macedonian Professor, Immunologist and Physician\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who won the 2023 U23 world wresting champs (men's freestyle 92 kg)\",\n",
    "        {\n",
    "            \"reference\": \"Muhammed Gimri from turkey\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What's my first meeting on Friday?\",\n",
    "        {\n",
    "            \"reference\": 'Your first meeting is 8:30 AM for \"Team Standup\"',\n",
    "            \"expected_steps\": [\"check_calendar\"],  # Only expect calendar tool\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "uid = uuid.uuid4()\n",
    "dataset_name = f\"Agent Eval Example {uid}\"\n",
    "ds = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"An example agent evals dataset using search and calendar checks.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q[0]} for q in questions],\n",
    "    outputs=[q[1] for q in questions],\n",
    "    dataset_id=ds.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc0a4c-e9a9-490c-91af-c702e35fad08",
   "metadata": {},
   "source": [
    "## 2. Define agent\n",
    "\n",
    "The main components of an agentic program are:\n",
    "- The agent (or runnable) that accepts the query and intermediate and responds with the next action to take\n",
    "- The tools the agent has access to\n",
    "- The executor, which controls the looping behavior when choosing subsequent actions\n",
    "\n",
    "In this example, we will create an agent with access to a DuckDuckGo search client (for informational search) and a mock tool to check a user's calendar for a given date.\n",
    "\n",
    "Our agent will use OpenAI function calling to ensure it generates arguments that conform to the tool's expected input schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.utils.function_calling import format_tool_to_openai_function\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_calendar(date: str) -> list:\n",
    "    \"\"\"Check the user's calendar for a meetings on the specified datetime (in iso format).\"\"\"\n",
    "    date_time = parse(date)\n",
    "    # A placeholder to demonstrate with multiple tools.\n",
    "    # It's easy to mock tools when testing.\n",
    "    if date_time.weekday() == 4:\n",
    "        return [\n",
    "            \"8:30 : Team Standup\",\n",
    "            \"9:00 : 1 on 1\",\n",
    "            \"9:45 design review\",\n",
    "        ]\n",
    "    return [\"Focus time\"]  # If only...\n",
    "\n",
    "\n",
    "def agent(inputs: dict):\n",
    "    llm = llm\n",
    "    tools = [\n",
    "        DuckDuckGoSearchResults(\n",
    "            name=\"duck_duck_go\"\n",
    "        ),  # General internet search using DuckDuckGo\n",
    "        check_calendar,\n",
    "    ]\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant.\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            (\"user\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    runnable_agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "    executor = AgentExecutor(\n",
    "        agent=runnable_agent,\n",
    "        tools=tools,\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    return executor.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d318",
   "metadata": {},
   "source": [
    "## 3. Define evaluators\n",
    "\n",
    "We will create a custom run evaluator to check the agent trajectory.\n",
    "It compares the run's intermediate steps against the \"ground truth\" we saved in the dataset above.\n",
    "\n",
    "Review the code below. Note that this evaluator expects the agent's response to contain the \"intermediate_steps\" key containing the list of agent actions. This is done by setting `return_intermediate_steps=True` above.\n",
    "\n",
    "This also expects your dataset to have the \"expected_steps\" key in each example outputs, as done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def intermediate_step_correctness(run: Run, example: Optional[Example] = None) -> dict:\n",
    "    if run.outputs is None:\n",
    "        raise ValueError(\"Run outputs cannot be None\")\n",
    "    # This is the output of each run\n",
    "    intermediate_steps = run.outputs.get(\"intermediate_steps\") or []\n",
    "    # Since we are comparing to the tool names, we now need to get that\n",
    "    # Intermediate steps is a Tuple[AgentAction, Any]\n",
    "    # The first element is the action taken\n",
    "    # The second element is the observation from taking that action\n",
    "    trajectory = [action.tool for action, _ in intermediate_steps]\n",
    "    # This is what we uploaded to the dataset\n",
    "    expected_trajectory = example.outputs[\"expected_steps\"]\n",
    "    # Just score it based on whether it is correct or not\n",
    "    score = int(trajectory == expected_trajectory)\n",
    "    return {\"key\": \"Intermediate steps correctness\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568f90",
   "metadata": {},
   "source": [
    "## 4. Evaluate\n",
    "\n",
    "Add your custom evaluator to the `custom_evaluators` list in the evaluation configuration below.\n",
    "\n",
    "Since our dataset has multiple output keys, we have to instruct the `run_on_dataset` function on which key to use as the ground truth for the QA evaluator by setting `reference_key=\"reference\"` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5201d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Evaluation with the <class 'langchain.evaluation.qa.eval_chain.QAEvalChain'> requires a language model to function. Failed to create the default 'gpt-4' model. Please manually provide an evaluation LLM or check your openai credentials.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/evaluation/loading.py:150\u001b[0m, in \u001b[0;36mload_evaluator\u001b[0;34m(evaluator, llm, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import langchain_openai or fallback onto \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community. Please install langchain_openai \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecify a language model explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m             )\n\u001b[0;32m--> 150\u001b[0m     llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_openai/chat_models/base.py:577\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: example\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: run\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m: example\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Measures whether a QA response is \"Correct\", based on a reference answer\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m qa_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mLangChainStringEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepare_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m chain_results \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     16\u001b[0m     agent,\n\u001b[1;32m     17\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     max_concurrency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py:165\u001b[0m, in \u001b[0;36mLangChainStringEvaluator.__init__\u001b[0;34m(self, evaluator, config, prepare_data)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluator, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_evaluator  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;241m=\u001b[39m \u001b[43mload_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported evaluator type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(evaluator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/evaluation/loading.py:154\u001b[0m, in \u001b[0;36mload_evaluator\u001b[0;34m(evaluator, llm, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m ChatOpenAI(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    151\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    152\u001b[0m         )\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage model to function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Failed to create the default \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please manually provide an evaluation LLM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or check your openai credentials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluator_cls\u001b[38;5;241m.\u001b[39mfrom_llm(llm\u001b[38;5;241m=\u001b[39mllm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Evaluation with the <class 'langchain.evaluation.qa.eval_chain.QAEvalChain'> requires a language model to function. Failed to create the default 'gpt-4' model. Please manually provide an evaluation LLM or check your openai credentials."
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "\n",
    "# We now need to specify this because we have multiple outputs in our dataset\n",
    "def prepare_data(run: Run, example: Example) -> dict:\n",
    "    return {\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"reference\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", prepare_data=prepare_data)\n",
    "chain_results = evaluate(\n",
    "    agent,\n",
    "    data=dataset_name,\n",
    "    evaluators=[intermediate_step_correctness, qa_evaluator],\n",
    "    experiment_prefix=\"Agent Eval Example\",\n",
    "    max_concurrency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe01d96-bfde-4979-a1e3-c6c70759d9b7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've succesfully performed a simple evaluation of the agent's trajectory by comparing it to an expected sequence of actions. This is useful when you know the expected steps to take. \n",
    "\n",
    "Once you've configured a custom evaluator for this type of evaluation, it's easy to apply other techniques using off-the-shelf evaluators like LangChain's [TrajectoryEvalChain](https://python.langchain.com/docs/guides/productionization/evaluation/trajectory/trajectory_eval#evaluate-trajectory), which can instruct an LLM to grade the efficacy of the agent's actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
